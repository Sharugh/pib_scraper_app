import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os

# Keywords of interest
KEYWORDS = [
    "energy", "refinery", "oil", "gas", "petroleum", "power", "electricity",
    "LNG", "coal", "renewable", "solar", "wind", "thermal", "exploration", "E&P",
    "production", "downstream", "upstream", "biofuel", "hydrocarbon", "ethanol", "pipeline"
]

# Ministries of interest
TARGET_MINISTRIES = [
    "Ministry of Petroleum & Natural Gas",
    "Ministry of Power",
    "Ministry of New and Renewable Energy",
    "Ministry of Chemicals and Fertilizers",
    "Ministry of Planning",
    "Ministry of Road Transport and Highways"
]

BASE_URL = "https://pib.gov.in/PressReleasePage.aspx"

# Load previous data
def load_previous_data():
    if os.path.exists("press_releases.xlsx"):
        return pd.read_excel("press_releases.xlsx")
    else:
        return pd.DataFrame(columns=["Date", "Title", "Link", "Ministry", "Matched Keywords"])

# Save to Excel
def save_data(df):
    df.to_excel("press_releases.xlsx", index=False)

# Extract press releases from PIB
def scrape_press_releases():
    releases = []
    for page_num in range(1, 5):  # You can increase this to scrape more pages
        url = f"{BASE_URL}?PRID=&Page={page_num}&MenuId=3"
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")
        press_items = soup.find_all("div", class_="content-area")

        for item in press_items:
            try:
                title_tag = item.find("a")
                title = title_tag.text.strip()
                link = "https://pib.gov.in/" + title_tag['href']
                date = item.find("span", class_="date").text.strip()
                ministry = item.find("div", class_="ministry").text.strip()

                if ministry in TARGET_MINISTRIES:
                    matched_keywords = [kw for kw in KEYWORDS if kw.lower() in title.lower()]
                    if matched_keywords:
                        releases.append({
                            "Date": date,
                            "Title": title,
                            "Link": link,
                            "Ministry": ministry,
                            "Matched Keywords": ", ".join(matched_keywords)
                        })
            except Exception as e:
                continue
    return releases

# Streamlit UI
def main():
    st.title("üîé PIB Energy Press Release Tracker")
    st.markdown("Scrapes the Indian Government's PIB website for energy-related updates.")

    if st.button("Scrape PIB Now"):
        st.info("Scraping latest data...")
        old_df = load_previous_data()
        scraped = scrape_press_releases()
        new_df = pd.DataFrame(scraped)

        if new_df.empty:
            st.warning("No new press releases found.")
            return

        # Remove duplicates
        combined_df = pd.concat([new_df, old_df]).drop_duplicates(subset=["Link"])
        new_items = pd.merge(new_df, old_df, how='outer', indicator=True).query('_merge == "left_only"')
        
        save_data(combined_df)

        st.success(f"‚úÖ {len(new_df)} relevant press releases scraped.")
        st.subheader("üìå New Items Found:")
        st.dataframe(new_items[["Date", "Title", "Ministry", "Link", "Matched Keywords"]])

        with st.expander("üîÅ Full Dataset"):
            st.dataframe(combined_df)

        st.download_button(
            "üì• Download Excel",
            combined_df.to_excel(index=False, engine='openpyxl'),
            file_name="press_releases.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

if __name__ == "__main__":
    main()
